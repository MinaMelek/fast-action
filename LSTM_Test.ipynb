{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"LSTM_Test.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"pXeUf7JsXheF","colab_type":"text"},"source":["# Import Drive to use the uploaded data"]},{"cell_type":"code","metadata":{"id":"QXUh8xfVDeiW","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"650b0acc-513a-4593-a271-95cd2dbd0889","executionInfo":{"status":"ok","timestamp":1570628877191,"user_tz":-120,"elapsed":4222,"user":{"displayName":"Mina 3.Melek","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAWTZeaXL6pBID15v903jVhJLIYLmIkX1FgP2jL=s64","userId":"10444321559887690581"}}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')#, force_remount=True)\n","curDir =('/content/gdrive/My Drive/Colab Notebooks/DeepMovement_project')\n","!cat /content/gdrive/My\\ Drive/Colab\\ Notebooks/DeepMovement_project"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","cat: '/content/gdrive/My Drive/Colab Notebooks/DeepMovement_project': Is a directory\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Jw8EUN2DDvql","colab_type":"code","outputId":"9b784bde-39b8-4263-831b-d86a82ea662c","executionInfo":{"status":"ok","timestamp":1570628526449,"user_tz":-120,"elapsed":75183,"user":{"displayName":"Mina 3.Melek","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAWTZeaXL6pBID15v903jVhJLIYLmIkX1FgP2jL=s64","userId":"10444321559887690581"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import tensorflow as tf\n","device_name = tf.test.gpu_device_name()\n","if device_name != '/device:GPU:0':\n","  raise SystemError('GPU device not found')\n","print('Found GPU at: {}'.format(device_name))"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Found GPU at: /device:GPU:0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-EH6vZGSX2IG","colab_type":"text"},"source":["# Setup the needed dependancies"]},{"cell_type":"code","metadata":{"id":"zqT3173EEWzD","colab_type":"code","outputId":"d704f7f0-7e97-4c7b-b5ac-5328cb917eff","executionInfo":{"status":"ok","timestamp":1570628904078,"user_tz":-120,"elapsed":14292,"user":{"displayName":"Mina 3.Melek","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAWTZeaXL6pBID15v903jVhJLIYLmIkX1FgP2jL=s64","userId":"10444321559887690581"}},"colab":{"base_uri":"https://localhost:8080/","height":836}},"source":["import os\n","from IPython.display import clear_output\n","os.chdir(curDir)\n","!pwd\n","# !cd /content/gdrive/My\\ Drive/Colab\\ Notebooks/Pose\\ project\n","!git clone https://www.github.com/ildoonet/tf-openpose\n","os.chdir('./tf-openpose')\n","!pip install -r requirements.txt\n","os.chdir(curDir)\n","clear_output"],"execution_count":4,"outputs":[{"output_type":"stream","text":["/content/gdrive/My Drive/Colab Notebooks/DeepMovement_project\n","fatal: destination path 'tf-openpose' already exists and is not an empty directory.\n","Collecting git+https://github.com/ppwwyyxx/tensorpack.git (from -r requirements.txt (line 13))\n","  Cloning https://github.com/ppwwyyxx/tensorpack.git to /tmp/pip-req-build-3ebmyo4p\n","  Running command git clone -q https://github.com/ppwwyyxx/tensorpack.git /tmp/pip-req-build-3ebmyo4p\n","Requirement already satisfied (use --upgrade to upgrade): tensorpack==0.9.8 from git+https://github.com/ppwwyyxx/tensorpack.git in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 13))\n","Requirement already satisfied: argparse in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 1)) (1.4.0)\n","Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 2)) (0.3.1.1)\n","Requirement already satisfied: fire in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 3)) (0.2.1)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 4)) (3.0.3)\n","Requirement already satisfied: numba in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 5)) (0.40.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 6)) (5.4.8)\n","Requirement already satisfied: pycocotools in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 7)) (2.0.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 8)) (2.21.0)\n","Requirement already satisfied: scikit-image in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 9)) (0.15.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 10)) (1.3.1)\n","Requirement already satisfied: slidingwindow in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 11)) (0.0.13)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 12)) (4.28.1)\n","Requirement already satisfied: numpy>=1.14 in /usr/local/lib/python3.6/dist-packages (from tensorpack==0.9.8->-r requirements.txt (line 13)) (1.16.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorpack==0.9.8->-r requirements.txt (line 13)) (1.12.0)\n","Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.6/dist-packages (from tensorpack==0.9.8->-r requirements.txt (line 13)) (1.1.0)\n","Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.6/dist-packages (from tensorpack==0.9.8->-r requirements.txt (line 13)) (0.8.5)\n","Requirement already satisfied: msgpack>=0.5.2 in /usr/local/lib/python3.6/dist-packages (from tensorpack==0.9.8->-r requirements.txt (line 13)) (0.5.6)\n","Requirement already satisfied: msgpack-numpy>=0.4.4.2 in /usr/local/lib/python3.6/dist-packages (from tensorpack==0.9.8->-r requirements.txt (line 13)) (0.4.4.3)\n","Requirement already satisfied: pyzmq>=16 in /usr/local/lib/python3.6/dist-packages (from tensorpack==0.9.8->-r requirements.txt (line 13)) (17.0.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements.txt (line 4)) (0.10.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements.txt (line 4)) (2.4.2)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements.txt (line 4)) (1.1.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements.txt (line 4)) (2.5.3)\n","Requirement already satisfied: llvmlite>=0.25.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba->-r requirements.txt (line 5)) (0.29.0)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->-r requirements.txt (line 8)) (1.24.3)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->-r requirements.txt (line 8)) (2.8)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->-r requirements.txt (line 8)) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->-r requirements.txt (line 8)) (2019.9.11)\n","Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->-r requirements.txt (line 9)) (1.0.3)\n","Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->-r requirements.txt (line 9)) (2.3)\n","Requirement already satisfied: imageio>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from scikit-image->-r requirements.txt (line 9)) (2.4.1)\n","Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->-r requirements.txt (line 9)) (4.3.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib->-r requirements.txt (line 4)) (41.2.0)\n","Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image->-r requirements.txt (line 9)) (4.4.0)\n","Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.3.0->scikit-image->-r requirements.txt (line 9)) (0.46)\n","Building wheels for collected packages: tensorpack\n","  Building wheel for tensorpack (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for tensorpack: filename=tensorpack-0.9.8-py2.py3-none-any.whl size=291857 sha256=2917f459058fb7a53cb17c8177e81322a0e45020811d0d299df1084d0d5b574b\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-qgerjtjf/wheels/a8/b5/a9/025b3a1294b9ffff93309e6956c65aa80e0fa40821d29eff1e\n","Successfully built tensorpack\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<function IPython.core.display.clear_output>"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"k4sZ3NGSHD-t","colab_type":"code","outputId":"d7dca7e0-4f6a-4ce7-f1a9-da58869ac3b6","executionInfo":{"status":"ok","timestamp":1570628960129,"user_tz":-120,"elapsed":16186,"user":{"displayName":"Mina 3.Melek","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAWTZeaXL6pBID15v903jVhJLIYLmIkX1FgP2jL=s64","userId":"10444321559887690581"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["os.chdir(curDir)\n","!apt-get install swig\n","os.chdir('./tf-openpose/tf_pose/pafprocess')\n","!pwd\n","!swig -python -c++ pafprocess.i && python3 setup.py build_ext --inplace\n","clear_output\n","os.chdir(curDir+'/tf-openpose')\n","!python setup.py install\n","clear_output\n","os.chdir(curDir)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","swig is already the newest version (3.0.12-1).\n","0 upgraded, 0 newly installed, 0 to remove and 8 not upgraded.\n","/content/gdrive/My Drive/Colab Notebooks/DeepMovement_project/tf-openpose/tf_pose/pafprocess\n","running build_ext\n","/content/gdrive/My: Operation not supported\n","running install\n","running bdist_egg\n","running egg_info\n","writing tf_pose.egg-info/PKG-INFO\n","writing dependency_links to tf_pose.egg-info/dependency_links.txt\n","writing requirements to tf_pose.egg-info/requires.txt\n","writing top-level names to tf_pose.egg-info/top_level.txt\n","file pafprocess.py (for module pafprocess) not found\n","writing manifest file 'tf_pose.egg-info/SOURCES.txt'\n","installing library code to build/bdist.linux-x86_64/egg\n","running install_lib\n","running build_py\n","file pafprocess.py (for module pafprocess) not found\n","copying tf_pose/pafprocess/pafprocess.py -> build/lib.linux-x86_64-3.6/tf_pose/pafprocess\n","file pafprocess.py (for module pafprocess) not found\n","running build_ext\n","creating build/bdist.linux-x86_64/egg\n","creating build/bdist.linux-x86_64/egg/tf_pose_data\n","copying build/lib.linux-x86_64-3.6/tf_pose_data/__init__.py -> build/bdist.linux-x86_64/egg/tf_pose_data\n","creating build/bdist.linux-x86_64/egg/tf_pose_data/graph\n","creating build/bdist.linux-x86_64/egg/tf_pose_data/graph/mobilenet_thin\n","copying build/lib.linux-x86_64-3.6/tf_pose_data/graph/mobilenet_thin/graph_opt.pb -> build/bdist.linux-x86_64/egg/tf_pose_data/graph/mobilenet_thin\n","creating build/bdist.linux-x86_64/egg/tf_pose\n","copying build/lib.linux-x86_64-3.6/tf_pose/__init__.py -> build/bdist.linux-x86_64/egg/tf_pose\n","copying build/lib.linux-x86_64-3.6/tf_pose/pose_augment.py -> build/bdist.linux-x86_64/egg/tf_pose\n","copying build/lib.linux-x86_64-3.6/tf_pose/networks.py -> build/bdist.linux-x86_64/egg/tf_pose\n","copying build/lib.linux-x86_64-3.6/tf_pose/train.py -> build/bdist.linux-x86_64/egg/tf_pose\n","copying build/lib.linux-x86_64-3.6/tf_pose/network_cmu.py -> build/bdist.linux-x86_64/egg/tf_pose\n","copying build/lib.linux-x86_64-3.6/tf_pose/pystopwatch.py -> build/bdist.linux-x86_64/egg/tf_pose\n","copying build/lib.linux-x86_64-3.6/tf_pose/datum_pb2.py -> build/bdist.linux-x86_64/egg/tf_pose\n","copying build/lib.linux-x86_64-3.6/tf_pose/network_mobilenet_thin.py -> build/bdist.linux-x86_64/egg/tf_pose\n","copying build/lib.linux-x86_64-3.6/tf_pose/network_mobilenet.py -> build/bdist.linux-x86_64/egg/tf_pose\n","copying build/lib.linux-x86_64-3.6/tf_pose/common.py -> build/bdist.linux-x86_64/egg/tf_pose\n","copying build/lib.linux-x86_64-3.6/tf_pose/network_dsconv.py -> build/bdist.linux-x86_64/egg/tf_pose\n","copying build/lib.linux-x86_64-3.6/tf_pose/network_base.py -> build/bdist.linux-x86_64/egg/tf_pose\n","copying build/lib.linux-x86_64-3.6/tf_pose/runner.py -> build/bdist.linux-x86_64/egg/tf_pose\n","copying build/lib.linux-x86_64-3.6/tf_pose/network_mobilenet_v2.py -> build/bdist.linux-x86_64/egg/tf_pose\n","copying build/lib.linux-x86_64-3.6/tf_pose/eval.py -> build/bdist.linux-x86_64/egg/tf_pose\n","copying build/lib.linux-x86_64-3.6/tf_pose/pose_dataset.py -> build/bdist.linux-x86_64/egg/tf_pose\n","copying build/lib.linux-x86_64-3.6/tf_pose/estimator.py -> build/bdist.linux-x86_64/egg/tf_pose\n","creating build/bdist.linux-x86_64/egg/tf_pose/mobilenet\n","copying build/lib.linux-x86_64-3.6/tf_pose/mobilenet/mobilenet.py -> build/bdist.linux-x86_64/egg/tf_pose/mobilenet\n","copying build/lib.linux-x86_64-3.6/tf_pose/mobilenet/mobilenet_v2.py -> build/bdist.linux-x86_64/egg/tf_pose/mobilenet\n","copying build/lib.linux-x86_64-3.6/tf_pose/mobilenet/__init__.py -> build/bdist.linux-x86_64/egg/tf_pose/mobilenet\n","copying build/lib.linux-x86_64-3.6/tf_pose/mobilenet/conv_blocks.py -> build/bdist.linux-x86_64/egg/tf_pose/mobilenet\n","creating build/bdist.linux-x86_64/egg/tf_pose/pafprocess\n","copying build/lib.linux-x86_64-3.6/tf_pose/pafprocess/setup.py -> build/bdist.linux-x86_64/egg/tf_pose/pafprocess\n","copying build/lib.linux-x86_64-3.6/tf_pose/pafprocess/__init__.py -> build/bdist.linux-x86_64/egg/tf_pose/pafprocess\n","copying build/lib.linux-x86_64-3.6/tf_pose/pafprocess/pafprocess.py -> build/bdist.linux-x86_64/egg/tf_pose/pafprocess\n","creating build/bdist.linux-x86_64/egg/tf_pose/slidingwindow\n","copying build/lib.linux-x86_64-3.6/tf_pose/slidingwindow/RectangleUtils.py -> build/bdist.linux-x86_64/egg/tf_pose/slidingwindow\n","copying build/lib.linux-x86_64-3.6/tf_pose/slidingwindow/Merging.py -> build/bdist.linux-x86_64/egg/tf_pose/slidingwindow\n","copying build/lib.linux-x86_64-3.6/tf_pose/slidingwindow/ArrayUtils.py -> build/bdist.linux-x86_64/egg/tf_pose/slidingwindow\n","copying build/lib.linux-x86_64-3.6/tf_pose/slidingwindow/SlidingWindow.py -> build/bdist.linux-x86_64/egg/tf_pose/slidingwindow\n","copying build/lib.linux-x86_64-3.6/tf_pose/slidingwindow/__init__.py -> build/bdist.linux-x86_64/egg/tf_pose/slidingwindow\n","copying build/lib.linux-x86_64-3.6/tf_pose/slidingwindow/WindowDistance.py -> build/bdist.linux-x86_64/egg/tf_pose/slidingwindow\n","copying build/lib.linux-x86_64-3.6/tf_pose/slidingwindow/Batching.py -> build/bdist.linux-x86_64/egg/tf_pose/slidingwindow\n","creating build/bdist.linux-x86_64/egg/tf_pose/tensblur\n","copying build/lib.linux-x86_64-3.6/tf_pose/tensblur/__init__.py -> build/bdist.linux-x86_64/egg/tf_pose/tensblur\n","copying build/lib.linux-x86_64-3.6/tf_pose/tensblur/smoother.py -> build/bdist.linux-x86_64/egg/tf_pose/tensblur\n","copying build/lib.linux-x86_64-3.6/_pafprocess.cpython-36m-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/egg\n","byte-compiling build/bdist.linux-x86_64/egg/tf_pose_data/__init__.py to __init__.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/tf_pose/__init__.py to __init__.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/tf_pose/pose_augment.py to pose_augment.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/tf_pose/networks.py to networks.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/tf_pose/train.py to train.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/tf_pose/network_cmu.py to network_cmu.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/tf_pose/pystopwatch.py to pystopwatch.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/tf_pose/datum_pb2.py to datum_pb2.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/tf_pose/network_mobilenet_thin.py to network_mobilenet_thin.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/tf_pose/network_mobilenet.py to network_mobilenet.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/tf_pose/common.py to common.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/tf_pose/network_dsconv.py to network_dsconv.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/tf_pose/network_base.py to network_base.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/tf_pose/runner.py to runner.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/tf_pose/network_mobilenet_v2.py to network_mobilenet_v2.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/tf_pose/eval.py to eval.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/tf_pose/pose_dataset.py to pose_dataset.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/tf_pose/estimator.py to estimator.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/tf_pose/mobilenet/mobilenet.py to mobilenet.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/tf_pose/mobilenet/mobilenet_v2.py to mobilenet_v2.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/tf_pose/mobilenet/__init__.py to __init__.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/tf_pose/mobilenet/conv_blocks.py to conv_blocks.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/tf_pose/pafprocess/setup.py to setup.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/tf_pose/pafprocess/__init__.py to __init__.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/tf_pose/pafprocess/pafprocess.py to pafprocess.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/tf_pose/slidingwindow/RectangleUtils.py to RectangleUtils.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/tf_pose/slidingwindow/Merging.py to Merging.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/tf_pose/slidingwindow/ArrayUtils.py to ArrayUtils.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/tf_pose/slidingwindow/SlidingWindow.py to SlidingWindow.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/tf_pose/slidingwindow/__init__.py to __init__.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/tf_pose/slidingwindow/WindowDistance.py to WindowDistance.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/tf_pose/slidingwindow/Batching.py to Batching.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/tf_pose/tensblur/__init__.py to __init__.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/tf_pose/tensblur/smoother.py to smoother.cpython-36.pyc\n","creating stub loader for _pafprocess.cpython-36m-x86_64-linux-gnu.so\n","byte-compiling build/bdist.linux-x86_64/egg/_pafprocess.py to _pafprocess.cpython-36.pyc\n","creating build/bdist.linux-x86_64/egg/EGG-INFO\n","copying tf_pose.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n","copying tf_pose.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n","copying tf_pose.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n","copying tf_pose.egg-info/not-zip-safe -> build/bdist.linux-x86_64/egg/EGG-INFO\n","copying tf_pose.egg-info/requires.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n","copying tf_pose.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n","writing build/bdist.linux-x86_64/egg/EGG-INFO/native_libs.txt\n","creating 'dist/tf_pose-0.1.1-py3.6-linux-x86_64.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n","removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n","Processing tf_pose-0.1.1-py3.6-linux-x86_64.egg\n","removing '/usr/local/lib/python3.6/dist-packages/tf_pose-0.1.1-py3.6-linux-x86_64.egg' (and everything under it)\n","creating /usr/local/lib/python3.6/dist-packages/tf_pose-0.1.1-py3.6-linux-x86_64.egg\n","Extracting tf_pose-0.1.1-py3.6-linux-x86_64.egg to /usr/local/lib/python3.6/dist-packages\n","tf-pose 0.1.1 is already the active version in easy-install.pth\n","\n","Installed /usr/local/lib/python3.6/dist-packages/tf_pose-0.1.1-py3.6-linux-x86_64.egg\n","Processing dependencies for tf-pose==0.1.1\n","error: tqdm 4.28.1 is installed but tqdm>4.29.0 is required by {'tensorpack'}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"aqrwzM_LYD08","colab_type":"text"},"source":["# Code Beginning"]},{"cell_type":"code","metadata":{"id":"8ChKmnqGECtu","colab_type":"code","outputId":"e6144e6c-bac1-4e46-b500-638432b0b1be","executionInfo":{"status":"ok","timestamp":1570629002541,"user_tz":-120,"elapsed":5218,"user":{"displayName":"Mina 3.Melek","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAWTZeaXL6pBID15v903jVhJLIYLmIkX1FgP2jL=s64","userId":"10444321559887690581"}},"colab":{"base_uri":"https://localhost:8080/","height":224}},"source":["# In[1]\n","import pandas as pd\n","import numpy as np\n","import pickle\n","import cv2\n","import os\n","import argparse\n","import logging\n","import sys\n","import time\n","from sklearn.preprocessing import MinMaxScaler\n","from tf_pose.estimator import TfPoseEstimator\n","from tf_pose.networks import get_graph_path, model_wh\n","from keras.models import load_model, model_from_json\n","# from google.colab import files\n","# src = list(files.upload().values())[0]\n","# open('UsedFunctions.py','wb').write(src)\n","os.chdir(curDir)\n","from UsedFunctions import *\n","from IPython.display import clear_output\n","%load_ext autoreload\n","%autoreload 2"],"execution_count":6,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:\n","The TensorFlow contrib module will not be included in TensorFlow 2.0.\n","For more information, please see:\n","  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n","  * https://github.com/tensorflow/addons\n","  * https://github.com/tensorflow/io (for I/O related ops)\n","If you depend on functionality not listed there, please file an issue.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tf_pose-0.1.1-py3.6-linux-x86_64.egg/tf_pose/mobilenet/mobilenet.py:369: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n","\n"],"name":"stdout"},{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"qzLF4XZjETqy","colab_type":"code","colab":{}},"source":["# In[2]\n","### Reading and processing the data for decision\n","def Regulate(framesInSeq):\n","    rem=[]\n","    for j in range(len(framesInSeq)):\n","        if framesInSeq[j] <= 7:\n","            if len(framesInSeq) <= 1:\n","                print(\"Error, very small file\")\n","                return None\n","            if j == 0 and (j+1) < len(framesInSeq):\n","                framesInSeq[j+1] += framesInSeq[j]\n","            elif j == len(framesInSeq)-1 and (j-1) >= 0:\n","                framesInSeq[j-1] += framesInSeq[j]\n","            else:\n","                if framesInSeq[j+1] < framesInSeq[j-1]:\n","                    framesInSeq[j+1] += framesInSeq[j]  \n","                else: framesInSeq[j-1] += framesInSeq[j]\n","            rem.append(j)\n","    framesInSeq=np.delete(framesInSeq,rem)\n","    return framesInSeq if framesInSeq.tolist() else None\n","\n","def ReadData(data, seq=50):\n","    df = pd.concat(data)  #read data\n","    \n","    #editing on data\n","    df.dropna(axis=0, how='any', thresh=76, subset=None, inplace=True)\n","    \n","    fr = df.copy()['Frame']\n","    #dropping usless columns and columns with all their values are 0\n","    dropped = ['Human', 'V_title', 'Frame', 'L Foot_x_PCM', 'L Foot_y_PCM', 'R Foot_L',\t'L Foot_L',\t'Head & Neck_R',\t'Trunk_R',\t'R Upper Arm_R',\t'L Upper Arm_R',\t'R Forearm_R',\t'L Forearm_R',\t'R Hand_R',\t'L Hand_R',\t'R Thigh_R',\t'L Thigh_R',\t'R Shank_R',\t'L Shank_R',\t'R Foot_R',\t'L Foot_R']\n","    df.drop(columns=dropped, inplace=True)\n","    \n","    seq_count = np.zeros(fr.shape, dtype=int)\n","    for i in range(1, len(fr.index)):\n","        # assuming if there is > 30 frames gap, the scene is changed\n","        if np.abs(fr.values[i-1] - fr.values[i]) > 30:\n","            seq_count[i] = seq_count[i-1] + 1\n","        else:\n","            seq_count[i] = seq_count[i-1]\n","    framesInSeq = np.bincount(seq_count)\n","    \n","    framesInSeq = Regulate(framesInSeq)\n","    if framesInSeq is None:\n","        return None\n","    nan_locations = np.isnan(np.float_(df.values)) # get locations of nan values to set to 0 later\n","    df.fillna(0, inplace=True) # convert NaN values to zeros\n","    \n","    array = df.values #conver df to array\n","    X = np.array(array, dtype='float')\n","    \n","    #feature scaling \n","    scaler = MinMaxScaler()\n","    X = scaler.fit_transform(X)\n","    X[nan_locations]=0 # set location of nans to 0\n","    \n","    Data_X = []\n","    for cLen in framesInSeq:\n","        for i in range((cLen // seq) + 1 * ((cLen % seq)!=0)):\n","            x = []\n","            for j in range(seq):\n","                if (seq*i + j) >= cLen:\n","                    x = np.tile(x, (seq//len(x)+1, 1))[:seq].tolist()\n","                    break\n","                else:\n","                    x.append(list(X[seq*i + j]))\n","            Data_X.append(x)\n","    return np.array(Data_X, dtype='float32')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"p1NfEHmAMhHd","colab_type":"code","colab":{}},"source":["# In[3]\n","\n","# logger = logging.getLogger('TfPoseEstimator')\n","# logger.setLevel(logging.DEBUG)\n","# ch = logging.StreamHandler()\n","# ch.setLevel(logging.DEBUG)\n","# formatter = logging.Formatter('[%(asctime)s] [%(name)s] [%(levelname)s] %(message)s')\n","# ch.setFormatter(formatter)\n","# logger.addHandler(ch)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dSCY2bQZMles","colab_type":"code","colab":{}},"source":["# In[4]\n","### Init\n","\"\"\" write your own path here \"\"\"\n","video_path = './Actions_set/running/person12_running_d2_uncomp.avi'#'./samples/P1/2001.mp4'\n","\n","resolution = '432x368'\n","poseModel = 'mobilenet_thin'\n","show_process = False\n","resize = '0x0'\n","showBG = True\n","resize_out_ratio = 4.0\n","\n","### Adding argument variables\n","# parser = argparse.ArgumentParser(description='tf-pose-estimation Video')\n","# parser.add_argument('--video-path', type=str, default=video_path,\n","#                     help='if provided, select videos in that directory. ')\n","# parser.add_argument('--resolution', type=str, default='432x368', help='network input resolution. default=432x368')\n","# parser.add_argument('--poseModel', type=str, default='mobilenet_thin', help='cmu / mobilenet_thin')\n","# parser.add_argument('--show-process', type=bool, default=False,\n","#                     help='for debug purpose, if enabled, speed for inference is dropped.')\n","# parser.add_argument('--resize', type=str, default='0x0',\n","#                     help='if provided, resize images before they are processed. default=0x0, Recommends : 432x368 or 656x368 or 1312x736 ')\n","# parser.add_argument('--showBG', type=bool, default=True, help='False to show skeleton only.')\n","# parser.add_argument('--resize-out-ratio', type=float, default=4.0,\n","#                     help='if provided, resize heatmaps before they are post-processed. default=1.0')\n","# #parser.add_argument('--mode', type=str, default='test',\n","# #                    help='if provided, choose the run mode to be eather train or test. default=test ')\n","# args = parser.parse_args()\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QFzaoVhrYVIq","colab_type":"text"},"source":["## Capture Video frames"]},{"cell_type":"code","metadata":{"id":"JN54MjrVdzsd","colab_type":"code","outputId":"6f9822f6-7f2a-4783-b526-ea299672759f","executionInfo":{"status":"ok","timestamp":1570629052855,"user_tz":-120,"elapsed":619,"user":{"displayName":"Mina 3.Melek","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAWTZeaXL6pBID15v903jVhJLIYLmIkX1FgP2jL=s64","userId":"10444321559887690581"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# In[5]\n","### Capturing the video frames\n","beg = time.time()             # For time computing\n","\n","# logger.debug('initialization %s : %s' % (poseModel, get_graph_path(poseModel)))\n","\n","w, h = model_wh(resize)\n","if w == 0 or h == 0:\n","\te = TfPoseEstimator(get_graph_path(poseModel), target_size=(432, 368))\n","else:\n","\te = TfPoseEstimator(get_graph_path(poseModel), target_size=(w, h))\n","\"\"\"        \n","newpath=os.getcwd()+'/Output'+'/'\n","if not os.path.exists(newpath):\n","    os.makedirs(newpath)\n","OUTPUT_PATH=newpath+'/'+'Humans_'+mode+'.csv'\n","\"\"\"\n","\n","OUTPUTS = []\n","\n","video_timing=time.time()             # For time computing\n","print(\"Reading video file \", video_path)\n","\n","cap = cv2.VideoCapture(video_path)\n","ret_val = True\n","if cap.isOpened() is False:\n","    print(\"Error opening video stream or file\")\n","count = 0\n","human_prev = None #@\n","while ret_val: # True as long as there are frames\n","    \n","    begin_counting=time.time()             # For time computing\n","    ret_val, image = cap.read()\n","    if not ret_val:\n","        break\n","    print (\"Frame No: \" , count)\n","    count += 1\n","    if count%2:\n","        print('skipping frame ')\n","        continue\n","    # Saving Frame\n","    #start=time.time()\n","    #cv2.imwrite(\"Output/\"+h_name+\"_Frame_No_\"+str(count)+\".jpg\",image)\n","    #print('Time taken for writing a Frame is {:.3f} ms'.format((time.time()-start)*1000))\n","    \n","    if image is None:\n","        logger.error('Image can not be read, path=%s' % video_path)\n","        sys.exit(-1)\n","    \n","    humans = e.inference(image, resize_to_default=(w > 0 and h > 0), upsample_size=resize_out_ratio)\n","    #humans = e.inference(image)\n","    print (humans)\n","    if len(humans)==0 or len(humans)>1:\n","        print(\"Inconvenient frame\")\n","        continue\n","\n","    Co_ordinates=manipulate(humans, human_prev, frame=count-1, mode='test')     # Computes the Co-ordinates from the given data by O-Nect\n","    human_prev = Co_ordinates.iloc[0].copy()#@\n","    \n","    #print('Time taken for Co-ordinate calculations for {} Frames is {:.3f} ms'.format(len(Co_ordinates),(time.time()-begin_counting)*1000))\n","    \n","    # Extracting X, Y Coordinates\n","    #start=time.time()\n","    X_Coords, Y_Coords= Get_Coords(Co_ordinates)\n","    #print('Time taken for Co-ordinate extractions for {} Frames is {:.3f} ms'.format(len(Co_ordinates),(time.time()-start)*1000))\n","    \n","    # Adding more features based on X, Y Coordinates\n","    #start=time.time()\n","    PCM_Frames= Calculate_PCM(X_Coords, Y_Coords)\n","    #print('Time taken for PCM for {} Frames is {:.3f} ms'.format(len(PCM_Frames),(time.time()-start)*1000))\n","    \n","    #start=time.time()\n","    TCM_x, TCM_y= Calculate_TCM(PCM_Frames)\n","    #print('Time taken for TCM for {} Frames is {:.3f} ms'.format(len(TCM_x),(time.time()-start)*1000))\n","    \n","    #start=time.time()\n","    L= Calculate_L(TCM_x, TCM_y, PCM_Frames)\n","    #print('Time taken for L features for {} Frames is {:.3f} ms'.format(len(PCM_Frames),(time.time()-start)*1000))\n","    \n","    #start=time.time()\n","    D1, D2, D3 = Calculate_D(PCM_Frames, TCM_x, TCM_y, 'Degrees')\n","    #print('Time taken for D1, D2, D3 features for {} Frames is {:.3f} ms'.format(len(PCM_Frames),(time.time()-start)*1000))\n","    \n","    #start=time.time()\n","    R=Calculate_R(PCM_Frames)\n","    #print('Time taken for R feature for {} Frames is {:.3f} ms'.format(len(PCM_Frames),(time.time()-start)*1000))\n","    \n","    #start=time.time()\n","    out=Add_Features_To_dataframe(Co_ordinates, PCM_Frames, TCM_x, TCM_y, L, R, D1, D2, D3)\n","    #print('Time taken for adding features to dataframe for {} Frames is {:.3f} ms'.format(len(Co_ordinates),(time.time()-start)*1000))\n","    \n","    print('Time taken for the whole file of {} Frames is {:.3f} ms'.format(len(PCM_Frames),(time.time()-begin_counting)*1000))\n","    OUTPUTS.append(out)\n","    clear_output()"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Frame No:  250\n","skipping frame \n","Frame No:  251\n","[]\n","Inconvenient frame\n","Frame No:  252\n","skipping frame \n","Frame No:  253\n","[]\n","Inconvenient frame\n","Frame No:  254\n","skipping frame \n","Frame No:  255\n","[]\n","Inconvenient frame\n","Frame No:  256\n","skipping frame \n","Frame No:  257\n","[]\n","Inconvenient frame\n","Frame No:  258\n","skipping frame \n","Frame No:  259\n","[]\n","Inconvenient frame\n","Frame No:  260\n","skipping frame \n","Frame No:  261\n","[]\n","Inconvenient frame\n","Frame No:  262\n","skipping frame \n","Frame No:  263\n","[]\n","Inconvenient frame\n","Frame No:  264\n","skipping frame \n","Frame No:  265\n","[]\n","Inconvenient frame\n","Frame No:  266\n","skipping frame \n","Frame No:  267\n","[]\n","Inconvenient frame\n","Frame No:  268\n","skipping frame \n","Frame No:  269\n","[]\n","Inconvenient frame\n","Frame No:  270\n","skipping frame \n","Frame No:  271\n","[]\n","Inconvenient frame\n","Frame No:  272\n","skipping frame \n","Frame No:  273\n","[]\n","Inconvenient frame\n","Frame No:  274\n","skipping frame \n","Frame No:  275\n","[]\n","Inconvenient frame\n","Frame No:  276\n","skipping frame \n","Frame No:  277\n","[]\n","Inconvenient frame\n","Frame No:  278\n","skipping frame \n","Frame No:  279\n","[]\n","Inconvenient frame\n","Frame No:  280\n","skipping frame \n","Frame No:  281\n","[]\n","Inconvenient frame\n","Frame No:  282\n","skipping frame \n","Frame No:  283\n","[]\n","Inconvenient frame\n","Frame No:  284\n","skipping frame \n","Frame No:  285\n","[]\n","Inconvenient frame\n","Frame No:  286\n","skipping frame \n","Frame No:  287\n","[]\n","Inconvenient frame\n","Frame No:  288\n","skipping frame \n","Frame No:  289\n","[]\n","Inconvenient frame\n","Frame No:  290\n","skipping frame \n","Frame No:  291\n","[]\n","Inconvenient frame\n","Frame No:  292\n","skipping frame \n","Frame No:  293\n","[]\n","Inconvenient frame\n","Frame No:  294\n","skipping frame \n","Frame No:  295\n","[]\n","Inconvenient frame\n","Frame No:  296\n","skipping frame \n","Frame No:  297\n","[]\n","Inconvenient frame\n","Frame No:  298\n","skipping frame \n","Frame No:  299\n","[]\n","Inconvenient frame\n","Frame No:  300\n","skipping frame \n","Frame No:  301\n","[]\n","Inconvenient frame\n","Frame No:  302\n","skipping frame \n","Frame No:  303\n","[]\n","Inconvenient frame\n","Frame No:  304\n","skipping frame \n","Frame No:  305\n","[]\n","Inconvenient frame\n","Frame No:  306\n","skipping frame \n","Frame No:  307\n","[]\n","Inconvenient frame\n","Frame No:  308\n","skipping frame \n","Frame No:  309\n","[]\n","Inconvenient frame\n","Frame No:  310\n","skipping frame \n","Frame No:  311\n","[]\n","Inconvenient frame\n","Frame No:  312\n","skipping frame \n","Frame No:  313\n","[]\n","Inconvenient frame\n","Frame No:  314\n","skipping frame \n","Frame No:  315\n","[]\n","Inconvenient frame\n","Frame No:  316\n","skipping frame \n","Frame No:  317\n","[]\n","Inconvenient frame\n","Frame No:  318\n","skipping frame \n","Frame No:  319\n","[]\n","Inconvenient frame\n","Frame No:  320\n","skipping frame \n","Frame No:  321\n","[]\n","Inconvenient frame\n","Frame No:  322\n","skipping frame \n","Frame No:  323\n","[]\n","Inconvenient frame\n","Frame No:  324\n","skipping frame \n","Frame No:  325\n","[]\n","Inconvenient frame\n","Frame No:  326\n","skipping frame \n","Frame No:  327\n","[]\n","Inconvenient frame\n","Frame No:  328\n","skipping frame \n","Frame No:  329\n","[]\n","Inconvenient frame\n","Frame No:  330\n","skipping frame \n","Frame No:  331\n","[]\n","Inconvenient frame\n","Frame No:  332\n","skipping frame \n","Frame No:  333\n","[]\n","Inconvenient frame\n","Frame No:  334\n","skipping frame \n","Frame No:  335\n","[]\n","Inconvenient frame\n","Frame No:  336\n","skipping frame \n","Frame No:  337\n","[]\n","Inconvenient frame\n","Frame No:  338\n","skipping frame \n","Frame No:  339\n","[]\n","Inconvenient frame\n","Frame No:  340\n","skipping frame \n","Frame No:  341\n","[]\n","Inconvenient frame\n","Frame No:  342\n","skipping frame \n","Frame No:  343\n","[]\n","Inconvenient frame\n","Frame No:  344\n","skipping frame \n","Frame No:  345\n","[]\n","Inconvenient frame\n","Frame No:  346\n","skipping frame \n","Frame No:  347\n","[]\n","Inconvenient frame\n","Frame No:  348\n","skipping frame \n","Frame No:  349\n","[]\n","Inconvenient frame\n","Frame No:  350\n","skipping frame \n","Frame No:  351\n","[]\n","Inconvenient frame\n","Frame No:  352\n","skipping frame \n","Frame No:  353\n","[]\n","Inconvenient frame\n","Frame No:  354\n","skipping frame \n","Frame No:  355\n","[]\n","Inconvenient frame\n","Frame No:  356\n","skipping frame \n","Frame No:  357\n","[]\n","Inconvenient frame\n","Frame No:  358\n","skipping frame \n","Frame No:  359\n","[]\n","Inconvenient frame\n","Frame No:  360\n","skipping frame \n","Frame No:  361\n","[]\n","Inconvenient frame\n","Frame No:  362\n","skipping frame \n","Frame No:  363\n","[]\n","Inconvenient frame\n","Frame No:  364\n","skipping frame \n","Frame No:  365\n","[]\n","Inconvenient frame\n","Frame No:  366\n","skipping frame \n","Frame No:  367\n","[]\n","Inconvenient frame\n","Frame No:  368\n","skipping frame \n","Frame No:  369\n","[]\n","Inconvenient frame\n","Frame No:  370\n","skipping frame \n","Frame No:  371\n","[]\n","Inconvenient frame\n","Frame No:  372\n","skipping frame \n","Frame No:  373\n","[]\n","Inconvenient frame\n","Frame No:  374\n","skipping frame \n","Frame No:  375\n","[]\n","Inconvenient frame\n","Frame No:  376\n","skipping frame \n","Frame No:  377\n","[]\n","Inconvenient frame\n","Frame No:  378\n","skipping frame \n","Frame No:  379\n","[]\n","Inconvenient frame\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"kF24vnW1YdNc","colab_type":"text"},"source":["## Model Reading"]},{"cell_type":"code","metadata":{"id":"GtMB8inmO0LM","colab_type":"code","outputId":"dc2ad1a6-2c6c-42a4-e62a-e7e49c723035","executionInfo":{"status":"ok","timestamp":1570629061998,"user_tz":-120,"elapsed":4964,"user":{"displayName":"Mina 3.Melek","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAWTZeaXL6pBID15v903jVhJLIYLmIkX1FgP2jL=s64","userId":"10444321559887690581"}},"colab":{"base_uri":"https://localhost:8080/","height":564}},"source":["# In[6]\n","### Prediction\n","\n","# load the mutual model variables from disk\n","modelspath=os.path.join(curDir, 'ModelData', 'Activity_P1')\n","filename = os.path.join(modelspath, 'LSTM_model_variables.sav')\n","with open(filename, 'rb') as handle:\n","    classes = pickle.load(handle)\n","    sequence_size = pickle.load(handle)\n","    batch_size = pickle.load(handle)\n","    lr = pickle.load(handle)\n","    decay = pickle.load(handle)\n","    lstm_out = pickle.load(handle)\n","    dense = pickle.load(handle)\n","#sequence_size = 10\n","#batch_size = 64\n","\n","# loading the model\n","#lstm_out = [128, 256, 512, 1024, 64]\n","#dense_layer = 128\n","#model = modelBuild(Data_X.shape[1:], lstm_out, [dense_layer, len(classes)])\n","\n","with open(os.path.join(modelspath,'model_architecture.json'), 'r') as f:\n","    model = model_from_json(f.read())\n","print(model.summary()) # optional\n"],"execution_count":10,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"],"name":"stdout"},{"output_type":"stream","text":["2019-10-09 13:50:58,399 WARNING From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"],"name":"stderr"},{"output_type":"stream","text":["Model: \"model_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_1 (InputLayer)         (None, 50, 79)            0         \n","_________________________________________________________________\n","lstm_1 (LSTM)                (None, 50, 128)           106496    \n","_________________________________________________________________\n","lstm_2 (LSTM)                (None, 50, 64)            49408     \n","_________________________________________________________________\n","lstm_3 (LSTM)                (None, 32)                12416     \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 128)               4224      \n","_________________________________________________________________\n","dropout_1 (Dropout)          (None, 128)               0         \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 3)                 387       \n","_________________________________________________________________\n","activation_1 (Activation)    (None, 3)                 0         \n","=================================================================\n","Total params: 172,931\n","Trainable params: 172,931\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ygLDMSN9c640","colab_type":"code","outputId":"1cef045e-39d5-43d2-f96a-825d7ed335b2","executionInfo":{"status":"error","timestamp":1570629071566,"user_tz":-120,"elapsed":1463,"user":{"displayName":"Mina 3.Melek","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAWTZeaXL6pBID15v903jVhJLIYLmIkX1FgP2jL=s64","userId":"10444321559887690581"}},"colab":{"base_uri":"https://localhost:8080/","height":197}},"source":["df = pd.concat(OUTPUTS)  #read data\n","\n","#editing on data\n","df.dropna(axis=0, how='any', thresh=76, subset=None, inplace=True)\n","\n","fr = df.copy()['Frame']\n","#dropping usless columns and columns with all their values are 0\n","dropped = ['Human', 'V_title', 'Frame', 'L Foot_x_PCM', 'L Foot_y_PCM', 'R Foot_L',\t'L Foot_L',\t'Head & Neck_R',\t'Trunk_R',\t'R Upper Arm_R',\t'L Upper Arm_R',\t'R Forearm_R',\t'L Forearm_R',\t'R Hand_R',\t'L Hand_R',\t'R Thigh_R',\t'L Thigh_R',\t'R Shank_R',\t'L Shank_R',\t'R Foot_R',\t'L Foot_R']\n","df.drop(columns=dropped, inplace=True)\n","\n","seq_count = np.zeros(fr.shape, dtype=int)\n","for i in range(1, len(fr.index)):\n","    # assuming if there is > 30 frames gap, the scene is changed\n","    if np.abs(fr.values[i-1] - fr.values[i]) > 30:\n","        seq_count[i] = seq_count[i-1] + 1\n","    else:\n","        seq_count[i] = seq_count[i-1]\n","framesInSeq = np.bincount(seq_count)\n","framesInSeq = Regulate(framesInSeq)\n","not framesInSeq.tolist()"],"execution_count":11,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-4f293c373aa6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mframesInSeq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbincount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mframesInSeq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRegulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframesInSeq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mnot\u001b[0m \u001b[0mframesInSeq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'tolist'"]}]},{"cell_type":"code","metadata":{"id":"FlEQfT3hCmyv","colab_type":"code","outputId":"6b008605-c9ae-4999-8df5-7205e331ef4c","executionInfo":{"status":"ok","timestamp":1562101632786,"user_tz":-120,"elapsed":3053,"user":{"displayName":"Mina 3.Melek","photoUrl":"https://lh5.googleusercontent.com/-OOPYvZH259E/AAAAAAAAAAI/AAAAAAAAAAs/QIC7DEH_sRk/s64/photo.jpg","userId":"10444321559887690581"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Reading data \n","# sequence_size = 50 ##@ Change later\n","Data_X= ReadData(OUTPUTS, sequence_size)\n","if Data_X is None:\n","    print('Skip this video')\n","else: print(Data_X.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Skip this video\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"kUhV68a3Yq1V","colab_type":"text"},"source":["## Test Results"]},{"cell_type":"code","metadata":{"id":"TQW6TOH1Qqhi","colab_type":"code","outputId":"82dc9391-8e34-4b7f-92c5-f2170b8695f9","executionInfo":{"status":"ok","timestamp":1562100562481,"user_tz":-120,"elapsed":4311,"user":{"displayName":"Mina 3.Melek","photoUrl":"https://lh5.googleusercontent.com/-OOPYvZH259E/AAAAAAAAAAI/AAAAAAAAAAs/QIC7DEH_sRk/s64/photo.jpg","userId":"10444321559887690581"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["# In[7]\n","model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","# Loading weights of the trained model \n","# chose = os.path.join(modelspath,'Chosen')\n","# for f in os.listdir(chose):\n","model.load_weights(os.path.join(modelspath, 'my_model_weights.hdf5'))\n","#model.load_weights(os.path.join(modelspath,'my_model_weights.hdf5'))\n","# print('for weights in %s' %f)\n","predicted = model.predict(Data_X, batch_size=batch_size).argmax(axis=1)\n","print('Predicted value is ', predicted)\n","print('The predicted person is most likly to be:')\n","Ps = set(predicted)\n","n = Ps\n","for f in range(len(Ps)):\n","    m = max(n, key=list(predicted).count)\n","    print('{}\\twith probability {:.2f}%'.format(classes[m], list(predicted).count(m)/len(predicted)*100))\n","    n.remove(m)\n","print('Total time taken is {:.3f} s'.format((time.time()-beg)))\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Predicted value is  [1 1 1 1]\n","The predicted person is most likly to be:\n","running\twith probability 100.00%\n","Total time taken is 1264.997 s\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"i29GyB7STqON","colab_type":"text"},"source":["#Other Tests"]},{"cell_type":"markdown","metadata":{"id":"F9cqnrGre0oS","colab_type":"text"},"source":["## Combined Method"]},{"cell_type":"code","metadata":{"id":"JqY1CXVWMuAc","colab_type":"code","outputId":"3fd87222-5ae9-4b8d-8ec9-8b8109fcd3ae","executionInfo":{"status":"ok","timestamp":1562105023819,"user_tz":-120,"elapsed":3174460,"user":{"displayName":"Mina 3.Melek","photoUrl":"https://lh5.googleusercontent.com/-OOPYvZH259E/AAAAAAAAAAI/AAAAAAAAAAs/QIC7DEH_sRk/s64/photo.jpg","userId":"10444321559887690581"}},"colab":{"base_uri":"https://localhost:8080/","height":71}},"source":["# In[5]\n","### Capturing the video frames\n","beg = time.time()             # For time computing\n","# logger.debug('initialization %s : %s' % (poseModel, get_graph_path(poseModel)))\n","\n","video_path = './Actions_set'\n","\n","resolution = '432x368'\n","poseModel = 'mobilenet_thin'\n","show_process = False\n","resize = '0x0'\n","showBG = True\n","resize_out_ratio = 4.0\n","\n","w, h = model_wh(resize)\n","if w == 0 or h == 0:\n","\te = TfPoseEstimator(get_graph_path(poseModel), target_size=(432, 368))\n","else:\n","\te = TfPoseEstimator(get_graph_path(poseModel), target_size=(w, h))\n","\"\"\"        \n","newpath=os.getcwd()+'/Output'+'/'\n","if not os.path.exists(newpath):\n","    os.makedirs(newpath)\n","OUTPUT_PATH=newpath+'/'+'Humans_'+mode+'.csv'\n","\"\"\"\n","valid_videos = [\".avi\",\".mp4\",\".mov\",\".mkv\",\".rmvb\", \".dv\", \".ts\"] \n","O =[]\n","F = []\n","cnt = 0\n","right = 0\n","for D in os.listdir(video_path):\n","    for f in os.listdir(os.path.join(video_path, D)):\n","        ext = os.path.splitext(f)[1]\n","        if ext.lower() not in valid_videos:\n","            continue\n","        OUTPUTS = []\n","\n","        video_timing=time.time()             # For time computing\n","        print(\"Reading video file \", f)\n","\n","        cap = cv2.VideoCapture(os.path.join(video_path, D, f))\n","        ret_val = True\n","        if cap.isOpened() is False:\n","            print(\"Error opening video stream or file\")\n","        count = 0\n","        human_prev = None #@\n","        while ret_val: # True as long as there are frames\n","\n","            begin_counting=time.time()             # For time computing\n","            ret_val, image = cap.read()\n","            if not ret_val:\n","                break\n","            #print (\"Frame No: \" , count)\n","            count += 1\n","            if count%2:\n","                #print('skipping frame ')\n","                continue\n","            # Saving Frame\n","            #start=time.time()\n","            #cv2.imwrite(\"Output/\"+h_name+\"_Frame_No_\"+str(count)+\".jpg\",image)\n","            #print('Time taken for writing a Frame is {:.3f} ms'.format((time.time()-start)*1000))\n","\n","            if image is None:\n","                logger.error('Image can not be read, path=%s' % video_path)\n","                sys.exit(-1)\n","\n","            humans = e.inference(image, resize_to_default=(w > 0 and h > 0), upsample_size=resize_out_ratio)\n","            #humans = e.inference(image)\n","            #print (humans)\n","            if len(humans)==0 or len(humans)>1:\n","                #print(\"Inconvenient frame\")\n","                continue\n","\n","            Co_ordinates=manipulate(humans, human_prev, frame=count-1, mode='test')     # Computes the Co-ordinates from the given data by O-Nect\n","            human_prev = Co_ordinates.iloc[0].copy()#@\n","\n","            #print('Time taken for Co-ordinate calculations for {} Frames is {:.3f} ms'.format(len(Co_ordinates),(time.time()-begin_counting)*1000))\n","\n","            # Extracting X, Y Coordinates\n","            #start=time.time()\n","            X_Coords, Y_Coords= Get_Coords(Co_ordinates)\n","            #print('Time taken for Co-ordinate extractions for {} Frames is {:.3f} ms'.format(len(Co_ordinates),(time.time()-start)*1000))\n","\n","            # Adding more features based on X, Y Coordinates\n","            #start=time.time()\n","            PCM_Frames= Calculate_PCM(X_Coords, Y_Coords)\n","            #print('Time taken for PCM for {} Frames is {:.3f} ms'.format(len(PCM_Frames),(time.time()-start)*1000))\n","\n","            #start=time.time()\n","            TCM_x, TCM_y= Calculate_TCM(PCM_Frames)\n","            #print('Time taken for TCM for {} Frames is {:.3f} ms'.format(len(TCM_x),(time.time()-start)*1000))\n","\n","            #start=time.time()\n","            L= Calculate_L(TCM_x, TCM_y, PCM_Frames)\n","            #print('Time taken for L features for {} Frames is {:.3f} ms'.format(len(PCM_Frames),(time.time()-start)*1000))\n","\n","            #start=time.time()\n","            D1, D2, D3 = Calculate_D(PCM_Frames, TCM_x, TCM_y, 'Degrees')\n","            #print('Time taken for D1, D2, D3 features for {} Frames is {:.3f} ms'.format(len(PCM_Frames),(time.time()-start)*1000))\n","\n","            #start=time.time()\n","            R=Calculate_R(PCM_Frames)\n","            #print('Time taken for R feature for {} Frames is {:.3f} ms'.format(len(PCM_Frames),(time.time()-start)*1000))\n","\n","            #start=time.time()\n","            out=Add_Features_To_dataframe(Co_ordinates, PCM_Frames, TCM_x, TCM_y, L, R, D1, D2, D3)\n","            #print('Time taken for adding features to dataframe for {} Frames is {:.3f} ms'.format(len(Co_ordinates),(time.time()-start)*1000))\n","\n","            #print('Time taken for the whole file of {} Frames is {:.3f} ms'.format(len(PCM_Frames),(time.time()-begin_counting)*1000))\n","            OUTPUTS.append(out)\n","        \n","        Data_X= ReadData(OUTPUTS, sequence_size)\n","        if Data_X is None:\n","            print('Skipping this video')\n","            continue\n","        print(Data_X.shape)\n","        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","        # Loading weights of the trained model \n","        # chose = os.path.join(modelspath,'Chosen')\n","        # for f in os.listdir(chose):\n","        model.load_weights(os.path.join(modelspath, 'my_model_weights.hdf5'))\n","        #model.load_weights(os.path.join(modelspath,'my_model_weights.hdf5'))\n","        # print('for weights in %s' %f)\n","        predicted = model.predict(Data_X, batch_size=batch_size).argmax(axis=1)\n","        print('Predicted value is ', predicted)\n","        print('The predicted person is most likly to be:')\n","        Ps = set(predicted)\n","        n = Ps\n","        for prop in range(len(Ps)):\n","            m = max(n, key=list(predicted).count)\n","            print('{}\\twith probability {:.2f}%'.format(classes[m], list(predicted).count(m)/len(predicted)*100))\n","            if prop == 0:\n","                TruePred = classes[m]\n","            n.remove(m)\n","        print()\n","        cnt += 1\n","        if TruePred == D:\n","            right += 1\n","        O.append(OUTPUTS)\n","        F.append(str(str(D)+'/'+str(f)))\n","        # In[6]\n","        ### Prediction\n","TotAccuracy = (right / cnt) * 100\n","print('Total accuracy for all the files is %.4f%%' %(TotAccuracy))\n","# # load the mutual model variables from disk\n","# modelspath=os.path.join(os.getcwd(),'ModelData')\n","# filename = os.path.join(modelspath, 'LSTM_model_variables.sav')\n","# with open(filename, 'rb') as handle:\n","#     classes = pickle.load(handle)\n","#     sequence_size = pickle.load(handle)\n","#     batch_size = pickle.load(handle)\n","# # loading the model\n","# #lstm_out = [128, 256, 512, 1024, 64]\n","# #dense_layer = 128\n","# #model = modelBuild(Data_X.shape[1:], lstm_out, [dense_layer, len(classes)])\n","\n","# with open(os.path.join(modelspath,'model_architecture.json'), 'r') as fmod:\n","#     Model = model_from_json(fmod.read())\n","# #         print(model.summary()) # optional\n","# # In[7]\n","# Model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","# # Loading weights of the trained model \n","# #         chose = os.path.join(modelspath,'Chosen')\n","# #         for f in os.listdir(chose):\n","# #sequence_size = 10\n","# #batch_size = 64\n","\n","print('Total time taken is {:.3f} s'.format((time.time()-beg)))\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[2019-10-09 13:51:45,951] [TfPoseEstimator] [INFO] loading graph from /usr/local/lib/python3.6/dist-packages/tf_pose-0.1.1-py3.6-linux-x86_64.egg/tf_pose_data/graph/mobilenet_thin/graph_opt.pb(default size=432x368)\n","2019-10-09 13:51:45,951 INFO loading graph from /usr/local/lib/python3.6/dist-packages/tf_pose-0.1.1-py3.6-linux-x86_64.egg/tf_pose_data/graph/mobilenet_thin/graph_opt.pb(default size=432x368)\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"wu_fFi8hix3u","colab_type":"text"},"source":["## Prediction with different weight files"]},{"cell_type":"code","metadata":{"id":"s1-wOHO0Nemq","colab_type":"code","colab":{}},"source":["chose = os.path.join(modelspath,'500_500')\n","for Wt in os.listdir(chose):\n","    ext = os.path.splitext(Wt)[1]\n","    if ext.lower() != '.hdf5':\n","        continue\n","    # Reading data \n","\n","    Model.load_weights(os.path.join(chose,Wt))\n","    #model.load_weights(os.path.join(chose,'my_model_weights.hdf5'))\n","\n","    for i in range(len(O)):\n","        Data_X= ReadData(O[i], sequence_size)\n","        predicted = Model.predict(Data_X, batch_size=batch_size).argmax(axis=1)\n","        print('for person in %s using weight file %s' %(F[i], Wt))\n","        print('Predicted value is ', predicted)\n","        print('The predicted person is most likly to be:')\n","        Ps = set(predicted)\n","        n = Ps\n","        for f in range(len(Ps)):\n","            m = max(n, key=list(predicted).count)\n","            print('{}\\twith probability {:.2f}%'.format(classes[m], list(predicted).count(m)/len(predicted)*100))\n","            n.remove(m)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DGZnGqK_e_uu","colab_type":"text"},"source":["### PlayGround"]},{"cell_type":"code","metadata":{"id":"oDi4xCULUOIg","colab_type":"code","colab":{}},"source":["for TT in os.listdir(os.path.join(modelspath,'Chosen')):\n","  print(TT)\n","clear_output()\n","print(TT)\n","print(os.listdir(video_path+'/P1'))\n","import glob\n","print(glob.glob(video_path+'/P1/*.mp4'))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"A2iuD8QgVk6T","colab_type":"code","colab":{}},"source":["print(O)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iT-cL64W6cNR","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}